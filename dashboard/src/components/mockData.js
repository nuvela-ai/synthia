// mockData.js
export const mockProjects = [
    {
      id: 'proj_1',
      name: 'Project 1',
      template: 'default',
      projects: 'projects', // This seems to be a category
      context: 'My new project description goes here',
      url: 'https://example.com/project1',
    },
    {
      id: 'proj_2',
      name: 'Project 2',
      template: 'custom',
      projects: 'projects',
      context: 'Another project description',
      url: 'https://example.com/project2',
    },
    {
      id: 'proj_3',
      name: 'Project 3',
      template: 'default',
      projects: 'projects',
      context: 'Third project description',
      url: 'https://example.com/project3',
    }
  ];
  
  export const mockFragments = [
    {
      id: 'frag_1',
      projectId: 'proj_1',
      name: 'Introduction to Unanswerability',
      author: 'Doe, John',
      year: '2022',
      template: 'default',
      category: 'projects',
      context: 'Unanswerability in question-answering frameworks refers to scenarios where a question cannot be answered based on the provided context. This is a common challenge in machine reading comprehension tasks, as models may hallucinate or generate incorrect answers when faced with unanswerable questions.',
      url: 'https://example.com/unanswerability-intro',
    },
    {
      id: 'frag_2',
      projectId: 'proj_1',
      name: 'Causes of Hallucination in LLMs',
      author: 'Smith, Jane',
      year: '2021',
      template: 'custom',
      category: 'projects',
      context: 'Hallucination in large language models (LLMs) occurs when the model generates plausible-sounding but factually incorrect or unsupported answers. This is often caused by the model\'s inability to recognize unanswerable questions, lack of grounding in the provided context, or over-optimization for fluency over accuracy.',
      url: 'https://example.com/hallucination-causes',
    },
    {
      id: 'frag_3',
      projectId: 'proj_1',
      name: 'Impact on Real-World Applications',
      author: 'Johnson, Alice',
      year: '2020',
      template: 'default',
      category: 'projects',
      context: 'In real-world applications, unanswerable questions and hallucination can lead to misinformation, reduced user trust, and potential harm. For example, in healthcare or legal domains, incorrect answers generated by LLMs could have serious consequences.',
      url: 'https://example.com/real-world-impact',
    },
    {
      id: 'frag_4',
      projectId: 'proj_1',
      name: 'Detecting Unanswerable Questions',
      author: 'Brown, Bob',
      year: '2019',
      template: 'custom',
      category: 'projects',
      context: 'Detecting unanswerable questions is a critical step in mitigating hallucination. Techniques include training models to recognize out-of-context queries, using confidence thresholds, and incorporating external knowledge bases to validate answers.',
      url: 'https://example.com/detecting-unanswerable',
    },
    {
      id: 'frag_5',
      projectId: 'proj_1',
      name: 'Dataset Design for Unanswerability',
      author: 'Green, Eve',
      year: '2018',
      template: 'default',
      category: 'projects',
      context: 'To address unanswerability, datasets for training QA models must include a mix of answerable and unanswerable questions. Examples include SQuAD 2.0, which explicitly includes unanswerable questions to help models learn to distinguish between answerable and unanswerable cases.',
      url: 'https://example.com/dataset-design',
    },
    {
      id: 'frag_6',
      projectId: 'proj_1',
      name: 'Model Architectures for Handling Unanswerability',
      author: 'Lee, Alex',
      year: '2017',
      template: 'custom',
      category: 'projects',
      context: 'Recent advancements in model architectures, such as multi-task learning and auxiliary classifiers, have shown promise in handling unanswerable questions. These approaches enable models to predict whether a question is answerable before generating a response.',
      url: 'https://example.com/model-architectures',
    },
    {
      id: 'frag_7',
      projectId: 'proj_1',
      name: 'Evaluation Metrics for Unanswerability',
      author: 'White, Sam',
      year: '2016',
      template: 'default',
      category: 'projects',
      context: 'Evaluating models on unanswerable questions requires specialized metrics, such as the F1 score for answerability prediction and the Exact Match (EM) score for answerable questions. These metrics help assess a model\'s ability to handle both answerable and unanswerable cases.',
      url: 'https://example.com/evaluation-metrics',
    },
    {
      id: 'frag_8',
      projectId: 'proj_1',
      name: 'Case Study: SQuAD 2.0',
      author: 'Grey, Max',
      year: '2015',
      template: 'custom',
      category: 'projects',
      context: 'SQuAD 2.0 is a widely used dataset for evaluating QA models on unanswerable questions. It includes over 50,000 questions, with a significant portion being unanswerable. Models trained on SQuAD 2.0 must learn to abstain from answering when no valid answer exists in the context.',
      url: 'https://example.com/squad2-case-study',
    },
    {
      id: 'frag_9',
      projectId: 'proj_1',
      name: 'Mitigating Hallucination with Reinforcement Learning',
      author: 'Blue, Sara',
      year: '2014',
      template: 'default',
      category: 'projects',
      context: 'Reinforcement learning techniques, such as reward modeling for abstention, can help mitigate hallucination in LLMs. By rewarding models for correctly identifying unanswerable questions, we can reduce the likelihood of generating incorrect answers.',
      url: 'https://example.com/reinforcement-learning',
    },
    {
      id: 'frag_10',
      projectId: 'proj_1',
      name: 'Future Directions in Unanswerability Research',
      author: 'Holden, James',
      year: '2013',
      template: 'custom',
      category: 'projects',
      context: 'Future research on unanswerability in QA frameworks should focus on improving model interpretability, integrating external knowledge sources, and developing more robust evaluation benchmarks. These advancements will help build more reliable and trustworthy QA systems.',
      url: 'https://example.com/future-directions',
    },
];
